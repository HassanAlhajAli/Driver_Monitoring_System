{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DMS Model v2 Demo. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a0e4c65a4b89f2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell in order to apply the necessary imports and to be able to use functions used by multiple cells."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6975012f82c13dd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Shared code snippets \"\"\"\n",
    "\n",
    "### Imports\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "###\n",
    "\n",
    "### Constants\n",
    "YOLOV8n_MODEL_PATH = 'models/yolov8n.pt'\n",
    "YOLOV8n_MODEL_PATH = 'runs/detect/train5/weights/best.pt'\n",
    "EYES_ONLY_MODEL_PATH = 'models/eyes.pt'\n",
    "DMS_V2_MODEL_PATH = 'models/dms_v2.pt'\n",
    "\n",
    "IMAGES_PATH = 'sample_tests/images'\n",
    "IMAGES_RESULTS_PATH = 'sample_tests/images/results'\n",
    "\n",
    "VIDEOS_PATH = 'sample_tests/videos'\n",
    "VIDEOS_RESULTS_PATH = 'sample_tests/videos/results'\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "### Functions\n",
    "\n",
    "# Resize camera/video frame to have consistent dimensions for all displays\n",
    "def resize_frame(frame):\n",
    "    # Define the desired width and height for displaying\n",
    "    desired_width = 800\n",
    "    desired_height = 600\n",
    "\n",
    "    # Get the original width and height of the image\n",
    "    original_height, original_width, _ = frame.shape\n",
    "\n",
    "    # Calculate the aspect ratio of the original image\n",
    "    aspect_ratio = original_width / original_height\n",
    "\n",
    "    # Calculate the new dimensions while preserving the aspect ratio\n",
    "    if aspect_ratio > 1:\n",
    "        # Landscape orientation\n",
    "        new_width = desired_width\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        # Portrait orientation or square\n",
    "        new_height = desired_height\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    # Resize the image to the new dimensions\n",
    "    resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "    return resized_frame\n",
    "\n",
    "\n",
    "def video_feed_and_results(model, video_source):\n",
    "    # Open the video file or one of the available camera devices\n",
    "    cap = video_source  # Change the camera index if needed\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Run YOLOv8 inference on the frame\n",
    "            results = model(frame)\n",
    "\n",
    "            # Visualize the results on the frame\n",
    "            annotated_frame = results[0].plot()\n",
    "            resized_image = resize_frame(annotated_frame)\n",
    "\n",
    "            # Display the annotated frame\n",
    "            cv2.imshow(\"DMS Model V2 Inference\", resized_image)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "493fdf568751b5e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First, let's try the original yolov8n model, which can detect 80 different classes. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a44a352a6c4bce96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the pretrained YOLOv8n model\n",
    "model = YOLO(YOLOV8n_MODEL_PATH)\n",
    "\n",
    "# Display model information\n",
    "model.info()\n",
    "\n",
    "# Get the list of object names (objects that the model is able to detect)\n",
    "object_names = model.names\n",
    "\n",
    "# Print the list of object names\n",
    "print(object_names)\n",
    "\n",
    "# Open the video file or one of the available camera devices\n",
    "cap = cv2.VideoCapture(1)  # Change the camera index if needed or specify video path\n",
    "\n",
    "video_feed_and_results(model=model, video_source=cap)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1ee005e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!yolo val model=runs/detect/train5/weights/best.pt data=/kaggle/input/final/data.yaml split=val"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe00c84dee81e812"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second, let's test the custom YOLOv8 model (dms-model-v2).\n",
    "#### The model can detect the following classes:\n",
    "- Open Eye\n",
    "- Closed Eye\n",
    "- Cigarette\n",
    "- Phone\n",
    "- Seatbelt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39f30bc90084ab28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Custom YOLOv8 Model\n",
    "model = YOLO(DMS_V2_MODEL_PATH)\n",
    "\n",
    "# Display model information\n",
    "model.info()\n",
    "\n",
    "# Get the list of object names (objects that the model is able to detect)\n",
    "object_names = model.names\n",
    "\n",
    "# Print the list of object names\n",
    "print(object_names)\n",
    "\n",
    "# Open the video file or one of the available camera devices\n",
    "cap = cv2.VideoCapture(1)  # Change the camera index if needed \n",
    "\n",
    "### Or specify a video path\n",
    "# video_file_name = 'v4.mp4'\n",
    "# cap = cv2.VideoCapture(f'{VIDEOS_PATH}/{video_file_name}')  # Change the camera index if needed or specify video path\n",
    "###\n",
    "\n",
    "video_feed_and_results(model=model, video_source=cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To save video results:\n",
    "(results will be saved at ./runs/detect/predict#/filename)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b64d06a458e693d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Custom YOLOv8 Model\n",
    "model = YOLO(YOLOV8n_MODEL_PATH)\n",
    "model.predict(source=f'{VIDEOS_PATH}/v5.mp4', save=True, conf=0.4, show=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12f34af295257495"
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's also possible to use a YOLO command to run inference on a video or camera input and save the results to a file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c881bfb3a3d51dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!yolo predict model=models/dms_v2.pt source=1 show=True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d4ec0ebd51671db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Third, testing the model on images and showing the results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0c5efe505c78d5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Custom YOLOv8 Model\n",
    "model = YOLO(DMS_V2_MODEL_PATH)\n",
    "\n",
    "img_file_name = '10.jpg'\n",
    "\n",
    "# Load the input image\n",
    "img = cv2.imread(f'{IMAGES_PATH}/{img_file_name}')\n",
    "\n",
    "# Run inference on an image\n",
    "results = model(img)  # results list\n",
    "\n",
    "# Visualize the results on the frame\n",
    "annotated_frame = results[0].plot()\n",
    "\n",
    "#Show the image with matplotlib\n",
    "plt.imshow(annotated_frame)\n",
    "plt.show()\n",
    "\n",
    "# View results\n",
    "for r in results:\n",
    "    print(r.boxes.data)  # print the Boxes object containing the detection bounding boxes\n",
    "\n",
    "# Show the results\n",
    "for r in results:\n",
    "    im_array = r.plot()  # plot a BGR numpy array of predictions\n",
    "    im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "    im.show()  # show image\n",
    "    im.save(f'{IMAGES_RESULTS_PATH}/results - {img_file_name}')  # save image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
